{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "One of the problems that we face at Govini is mapping entities between disparate data sets. We collect data from a variety of different sources, and knowing that a certain entity is the same from one data set to another is essential.\n",
    "\n",
    "For example, there may be a company named \"FOO\" in one data set, and a company named \"foo 123\" in another. We need to be able to determine with a high enough confidence that those are the same entity. However, a majority of the time, the data does not share a unique key. \n",
    "\n",
    "Using the data available, as well as relationships and metadata, we map these together with a high precision. We need you to devise an algorithm that could automatically tie related entities together. The output should have the matches as well as a confidence score for the match.\n",
    "\n",
    "We are giving you two sample data sets, and it'll be up to you to generate a mapping between the two. In most cases, the mapping should be one to one. An ID from data set A should map to an ID in data set B. \n",
    "\n",
    "The sample data sets are available at the following link:\n",
    "https://s3.amazonaws.com/BUCKET_FOR_FILE_TRANSFER/interview.tar.xz\n",
    "\n",
    "# Data\n",
    "\n",
    "The archive contains five files, described below:\n",
    "\n",
    "## Procurement Data:\n",
    "\n",
    " * Company Information - data/mdl__dim_vendor.csv   (a__company.csv)\n",
    " * Location Information - data/mdl__dim_geo.csv     (a__geo.csv)\n",
    "\n",
    "mdl__dim_vendor.csv references mdl__dim_geo.csv via the column **geo_id**.\n",
    "\n",
    "## Finance Data:\n",
    "\n",
    " * Company Information - data/factset__ent_entity_coverage.csv  (b_company,csv)\n",
    " * Company Hierarchy - data/factset__ent_entity_structure.csv   (b__hierarchy.csv)\n",
    " * Location Information - data/factset__ent_entity_address.csv  (b__address.csv)\n",
    "\n",
    "All of these files are tied together using factset_entity_id (**b_entity_id**).\n",
    "\n",
    "The end goal of this exercise is to explore the data, and map mdl__dim_vendor.vendor_id to corresponding factset__ent_entity_coverage.factset_entity_id. Ideally, a file containing three columns: vendor_id, factset_entity_id, confidence_of_match. Please make sure there is a README file that explains how your algorithm works.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement Note\n",
    "The problem statement's decription of the filenames and the data doesn't *exactly* match the sample data that was downloaded.  Unfortunately, this is *typical* of real-world scenarios, but it's close enough to still be informative.  My updates to the problem statement appear in parentheticals.\n",
    "\n",
    "### The process of joining the datasets will be done in the following steps:\n",
    "\n",
    "0. Setup a data analytics environment (e.g. Jupyer Lab, dtools profiler) and acquire necessary the data\n",
    "1. Join a tables in geo_id and join b tables on b_entity_id\n",
    "3. Profile and explore the data to determine candidate matching columns\n",
    "4. Clean and normalize matching columns (e.g. upcase, truncation, normalize company name, normalize street address)\n",
    "5. Determine blocking functions on exact matches for speed (e.g. STATE_PROVINCE)\n",
    "6. Perform the matching\n",
    "7. Emit the results\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "The CSV data was profiled using my opensource dtools (https://github.com/scholarsmate/dtools) and helped to inform my choices on features that would be suitable for blocking and matching.  The data profile results are not part of this notebook, but can be provided on request.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Of The Matching Algorithms\n",
    "\n",
    "### Blocking\n",
    "The number of match record pairings using a naive matching approach of two tables is the number of records in the first table times the number of records in the second table.  A table with about 80,000 records being matched against another table of about 200,000 records results in 16,000,000,000 (16 trillion) pairings.  We need to vastly reduce the number of pairings to be able to perform the matching in a reasonable amount of time on modest hardware.  To reduce the pairings, we can create smaller \"buckets\" of records where only records in the same bucket are paired up for matching.\n",
    "\n",
    "#### Features Selected For Blocking\n",
    "For the given datasets, I chose to block on the *ISO 2 country code*, the *state/province code*, and the *first letter of the normalized company name*.  These are features that the two datasets had in common, were reasonably populated and, of the features that the datasets have in common, are typically reliable.\n",
    "\n",
    "### Matching\n",
    "Once the record pairs are blocked, we need to determine other common features that can be used for determining if a record pair should be matched and how fuzzy of a match we can tolerate.  For these datasets, I've used exact matching, where the values match only if they are identical, and Damerau–Levenshtein for fuzzy matching.  The Damerau–Levenshtein distance is the number of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters, that it takes to make one value an exact match of the other.  This distance is used to compute a similarity score where we get 1 in case of and exact match and 0 in the case of complete disagreement.\n",
    "\n",
    "#### Features Selected For Matching\n",
    "The common features that I chose for blocking are normalized versions of the company name, street address, city, postal code, phone number, and website.\n",
    "\n",
    "##### Feature Cleaning and Normalization\n",
    "To provide the best chance at feature matching, the features should be pre-processed.  In pre-processing the features are cleaned up and normalized.\n",
    "\n",
    "To normalize the company name, I used a python module called \"cleanco\" to remove the company's legal entity from the company name (e.g. \"Foo, LLC\" becomes \"Foo\").  To normalize the street address, I used a python module called \"scourgify\" that parses and rebuilds street addresses (e.g. \"123 southwest Main street\" becomes \"123 SW MAIN ST\").  Postal codes are truncated to the first 5 characters to handle matching of ZIP-5 with ZIP-9.  Phone numbers are processed by removing all non-numbers (except +).  Websites are lower-cased and those that don't begin with \"http\" get \"http://\" appended to them so that WWW.FOO.COM will match http://www.foo.com.  Company name, city and street characters are mapped into their lower-case ASCII characters, punctuation removed, white-space is elided and trimmed.\n",
    "\n",
    "### Exact-ish Match\n",
    "The first matching algorithm, I call \"exact-ish\" match because I'm performing exact matching on the normalized versions of the features used for matching.  If a feature is exactly matched in a record pair, that feature match scores a 1 and they don't match, it scores a 0.  This matching algorithm is very fast.  It's good for quick, high-confidence matches, but can miss matching pairs that *should* match due to minor variations in feature values.\n",
    "\n",
    "### Fuzzy Match\n",
    "The second matching algorithm uses fuzzy matching, which is slower than exact matching, but is able to put together more matching pairs because it is able to tolerate minor variations in feature values.  Fuzzy matching algorithms can be tuned for similarity, allowing more or less fuzziness for each feature comparison (e.g. allow more fuzziness for street address, but less fuzziness for ZIP-5 comparisons).  I chose an 80% similarity threshold for company name, city, postal code, phone number, and website, and I chose %75 similarity for street address.\n",
    "\n",
    "### Match Weights\n",
    "For features that match, they score a 1, and a 0 for those that don't match, but not all matches ought to be treated equal.  For example, in this case a company name match is more important (“weighs” more) than a postal code match.  To account for this, I've applied different weights to these matches in accordance with their relative importance.  I've also selected the weights so that they fit in a 0 - 100 range.  The weights are as follows:\n",
    "\n",
    "* 60 for company name matches\n",
    "* 20 for street address matches\n",
    "* 5 for city matches\n",
    "* 5 for postal code matches\n",
    "* 5 for phone number matches\n",
    "* 5 for website matches\n",
    "\n",
    "The sum of these weights determines the “confidence of match” where 100 is a perfect match and 0 is complete disagreement.\n",
    "\n",
    "### Match Threshold\n",
    "For a record pair to be considered a match, the confidence of match must exceed a given threshold.  I chose 75 because given the assigned weights, it must always have a company match, and it must also have a street address match, or at least 3 of the 4 remaining matches (city, postal code, phone number, and website) for the record pair to be considered a match.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np                           # BSD-3-Clause License\n",
    "import unidecode                             # GNU General Public License \n",
    "import pandas as pd                          # BSD 3-Clause\n",
    "import recordlinkage                         # BSD 3-Clause\n",
    "import recordlinkage.preprocessing           # BSD-3-Clause License\n",
    "import recordlinkage.standardise             # BSD-3-Clause License\n",
    "import scourgify                             # MIT License\n",
    "\n",
    "from werkzeug.urls import url_fix            # BSD-3-Clause License\n",
    "from cleanco import prepare_terms, basename  # MIT License\n",
    "\n",
    "#import qgrid                                 # Apache-2.0 License\n",
    "import dtale                                 # GNU Lesser General Public License v2.1\n",
    "\n",
    "# Company type terms for company name normalization\n",
    "terms = prepare_terms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normalized version of a street address (the portion of the address without the city, state and zip)\n",
    "def normalize_street_address(street_addr):\n",
    "    if isinstance(street_addr, str) and street_addr:\n",
    "        try:\n",
    "            return scourgify.normalize.get_addr_line_str(scourgify.normalize.normalize_addr_str(street_addr))\n",
    "        except:\n",
    "            # If the street address was not parseable, return the original\n",
    "            street_addr = street_addr.strip()\n",
    "            if street_addr:\n",
    "                return street_addr\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translates characters into ASCII, upper-cases, removes punctuation, elides and trims whitespace, then uses cleanco to just get the company's \"basename\"\n",
    "def normalize_company_name(name):\n",
    "    if isinstance(name, str) and name:\n",
    "        return basename(unidecode.unidecode(name), terms, prefix=False, middle=False, suffix=True)\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_city_name(city):\n",
    "    if isinstance(city, str) and city:\n",
    "        return unidecode.unidecode(city)\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_web_address(web_address):\n",
    "    if isinstance(web_address, str) and web_address:\n",
    "        web_address = web_address.lower()\n",
    "        if not web_address.startswith(\"http\"):\n",
    "            web_address = \"http://\" + web_address\n",
    "        if web_address.endswith(\"/\"):\n",
    "            web_address = web_address[:-1]\n",
    "        return url_fix(web_address)\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matching Function\n",
    "The Damerau–Levenshtein distance differs from the classical Levenshtein distance by including *transpositions* among its allowable operations in addition to the three classical single-character edit operations (insertions, deletions and substitutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min link_threshold is 5, max is 100\n",
    "def link_fuzzy(a_records, b_records, link_threshold=75):\n",
    "    if not 5 <= link_threshold <= 100:\n",
    "        raise ValueError()\n",
    "    comp = recordlinkage.Compare()\n",
    "    comp.string('NAME_A_NORM', 'NAME_B_NORM', method='damerau_levenshtein', threshold=0.80, label='name_norm_score')\n",
    "    comp.string('STREET_A_NORM', 'STREET_B_NORM', method='damerau_levenshtein', threshold=0.75, label='street_norm_score')\n",
    "    comp.string('CITY_A_NORM', 'CITY_B_NORM', method='damerau_levenshtein', threshold=0.80, label='city_norm_score')\n",
    "    comp.string('PHONE_A_NORM', 'PHONE_B_NORM', method='damerau_levenshtein', threshold=0.80, label='phone_norm_score')\n",
    "    comp.string(\"WEB_A_NORM\", \"WEB_B_NORM\", method='damerau_levenshtein', threshold=0.80, label='web_match_score')\n",
    "    comp.string(\"POSTAL_A_NORM\", \"POSTAL_B_NORM\", method='damerau_levenshtein', threshold=0.80, label='postal_code_score')\n",
    "\n",
    "    # Block by 2-letter country code, state and the first letter of the normalized company name\n",
    "    block_ix = recordlinkage.index.Block(left_on=['country_iso2', 'state', 'NAME_A_FIRST_LETTER'],\n",
    "                                         right_on=['iso_country', 'state_province', 'NAME_B_FIRST_LETTER']).index(a_records, b_records)\n",
    "    # Run the compare against all record pairs within a block\n",
    "    comp_results = comp.compute(block_ix, a_records, b_records)\n",
    "    \n",
    "    # Apply weights to the matching results\n",
    "    comp_results[\"name_norm_score\"] = comp_results.name_norm_score * 60\n",
    "    comp_results[\"street_norm_score\"] = comp_results.street_norm_score * 20\n",
    "    comp_results[\"city_norm_score\"] = comp_results.city_norm_score * 5\n",
    "    comp_results[\"phone_norm_score\"] = comp_results.phone_norm_score * 5\n",
    "    comp_results[\"web_match_score\"] = comp_results.web_match_score * 5\n",
    "    comp_results[\"postal_code_score\"] = comp_results.postal_code_score * 5\n",
    "    comp_results[\"confidence_of_match\"] = comp_results[[\"name_norm_score\", \"street_norm_score\", \"city_norm_score\", \"phone_norm_score\", \"web_match_score\", \"postal_code_score\"]].sum(axis=1)\n",
    "    \n",
    "    # Consider links to be those whose scores sum greater than or equal to the given link threshold\n",
    "    return comp_results[comp_results[\"confidence_of_match\"] >= link_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact-ish Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min link_threshold is 5, max is 100\n",
    "def link_exactish(a_records, b_records, link_threshold=75):\n",
    "    if not 5 <= link_threshold <= 100:\n",
    "        raise ValueError()\n",
    "    comp = recordlinkage.Compare()\n",
    "    comp.exact('NAME_A_NORM', 'NAME_B_NORM', label='name_norm_score')\n",
    "    comp.exact('STREET_A_NORM', 'STREET_B_NORM', label='street_norm_score')\n",
    "    comp.exact('CITY_A_NORM', 'CITY_B_NORM', label='city_norm_score')\n",
    "    comp.exact('PHONE_A_NORM', 'PHONE_B_NORM', label='phone_norm_score')\n",
    "    comp.exact(\"WEB_A_NORM\", \"WEB_B_NORM\", label='web_match_score')\n",
    "    comp.exact(\"POSTAL_A_NORM\", \"POSTAL_B_NORM\", label='postal_code_score')\n",
    "\n",
    "    # Block by state code, DOB, gender, ZIP 5 and phonetic first name to create pairs of candidate links\n",
    "    block_ix = recordlinkage.index.Block(left_on=['country_iso2', 'state', 'NAME_A_FIRST_LETTER'],\n",
    "                                         right_on=['iso_country', 'state_province', 'NAME_B_FIRST_LETTER']).index(a_records, b_records)\n",
    "    # Run the compare against all record pairs within a block\n",
    "    comp_results = comp.compute(block_ix, a_records, b_records)\n",
    "\n",
    "    # Apply weights to the matching results\n",
    "    comp_results[\"name_norm_score\"] = comp_results.name_norm_score * 60\n",
    "    comp_results[\"street_norm_score\"] = comp_results.street_norm_score * 20\n",
    "    comp_results[\"city_norm_score\"] = comp_results.city_norm_score * 5\n",
    "    comp_results[\"phone_norm_score\"] = comp_results.phone_norm_score * 5\n",
    "    comp_results[\"web_match_score\"] = comp_results.web_match_score * 5\n",
    "    comp_results[\"postal_code_score\"] = comp_results.postal_code_score * 5\n",
    "    comp_results[\"confidence_of_match\"] = comp_results[[\"name_norm_score\", \"street_norm_score\", \"city_norm_score\", \"phone_norm_score\", \"web_match_score\", \"postal_code_score\"]].sum(axis=1)\n",
    "    \n",
    "    # Consider links to be those whose scores sum greater than or equal to the given link threshold\n",
    "    return comp_results[comp_results[\"confidence_of_match\"] >= link_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the A-side company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 21:26:44,990 - INFO     - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-11-11 21:26:44,991 - INFO     - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe4216a0b20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a_company = pd.read_csv(\"data/a__company.csv\", index_col=\"vendor_id\", usecols=[\"vendor_id\", \"name\", \"address\", \"phone\", \"websiteurl\", \"geo_id\"])[[\"name\", \"address\", \"phone\", \"websiteurl\", \"geo_id\"]]\n",
    "#qgrid.show_grid(df_a_company)\n",
    "dtale.show(df_a_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the A-side location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe420c14520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a_geo = pd.read_csv(\"data/a__geo.csv\", index_col=\"geo_id\", usecols=[\"geo_id\", \"country_iso2\", \"city\", \"state\", \"zipcode\"], dtype={'zipcode': 'str'})[[\"country_iso2\", \"city\", \"state\", \"zipcode\"]]\n",
    "dtale.show(df_a_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the A-side company and location data into a single A-side table, then drop the geo_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.merge(df_a_company, df_a_geo, on=\"geo_id\", right_index=True) # left inner-join\n",
    "\n",
    "df_a.drop(columns=[\"geo_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and normalize the A-side matching features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe43c8d4880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a[\"NAME_A_NORM\"] = recordlinkage.standardise.clean(df_a.name.apply(normalize_company_name))\n",
    "df_a[\"NAME_A_FIRST_LETTER\"] = df_a.NAME_A_NORM.apply(lambda x: x[:1] if x else '')\n",
    "df_a[\"STREET_A_NORM\"] = recordlinkage.standardise.clean(df_a.address.apply(normalize_street_address))\n",
    "df_a[\"CITY_A_NORM\"] = recordlinkage.standardise.clean(df_a.city.apply(normalize_city_name))\n",
    "df_a[\"PHONE_A_NORM\"] = recordlinkage.standardise.phonenumbers(df_a.phone)\n",
    "df_a[\"WEB_A_NORM\"] = df_a.websiteurl.apply(normalize_web_address)\n",
    "df_a[\"POSTAL_A_NORM\"] = df_a.zipcode.apply(lambda x: x[:5].upper() if x and isinstance(x, str) else np.nan)\n",
    "\n",
    "dtale.show(df_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480\n",
      "76344\n",
      "76344\n"
     ]
    }
   ],
   "source": [
    "print(len(df_a_geo))\n",
    "print(len(df_a_company))\n",
    "print(len(df_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the B-side company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe420ea4220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b_company = pd.read_csv(\"data/b__company.csv\", index_col=\"b_entity_id\", usecols=[\"b_entity_id\", \"entity_proper_name\", \"web_site\"])[[\"entity_proper_name\", \"web_site\"]]\n",
    "dtale.show(df_b_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the B-side location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/5\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe4395ddf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b_address = pd.read_csv(\"data/b__address.csv\", index_col=\"b_entity_id\", usecols=[\"b_entity_id\", \"iso_country\", \"location_city\", \"state_province\", \"location_postal_code\", \"location_street1\", \"location_street2\", \"location_street3\", \"tele_full\"])[[\"iso_country\", \"location_city\", \"state_province\", \"location_postal_code\", \"location_street1\", \"location_street2\", \"location_street3\", \"tele_full\"]]\n",
    "dtale.show(df_b_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the B-side company and location data into a single B-side table, since there are now duplicate b_entity_ids, we need to create a new table index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = pd.merge(df_b_company, df_b_address, on=\"b_entity_id\", how=\"left\")\n",
    "\n",
    "df_b = df_b.reset_index()\n",
    "df_b.index.names=[\"pk_b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and normalize the B-side matching features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196498\n",
      "200239\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/6\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe4285b6fa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can't concatenate with nan's, so change these to empty strings\n",
    "df_b.location_street1 = df_b.location_street1.fillna('')\n",
    "df_b.location_street2 = df_b.location_street2.fillna('')\n",
    "df_b.location_street3 = df_b.location_street3.fillna('')\n",
    "\n",
    "df_b[\"NAME_B_NORM\"] = recordlinkage.standardise.clean(df_b.entity_proper_name.apply(normalize_company_name))\n",
    "df_b[\"STREET_B\"] = df_b.location_street1 + \" \" + df_b.location_street2 + \" \" + df_b.location_street3\n",
    "df_b[\"NAME_B_FIRST_LETTER\"] = df_b.NAME_B_NORM.apply(lambda x: x[:1] if x else '')\n",
    "df_b[\"STREET_B_NORM\"] = recordlinkage.standardise.clean(df_b.STREET_B.apply(normalize_street_address))\n",
    "df_b[\"CITY_B_NORM\"] = recordlinkage.standardise.clean(df_b.location_city.apply(normalize_city_name))\n",
    "df_b[\"PHONE_B_NORM\"] = recordlinkage.standardise.phonenumbers(df_b.tele_full)\n",
    "df_b[\"WEB_B_NORM\"] = df_b.web_site.apply(normalize_web_address)\n",
    "df_b[\"POSTAL_B_NORM\"] = df_b.location_postal_code.apply(lambda x: x[:5].upper() if x and isinstance(x, str) else np.nan)\n",
    "\n",
    "df_b.drop(columns=[\"STREET_B\"], inplace=True)\n",
    "\n",
    "print(len(df_b_company))\n",
    "print(len(df_b))\n",
    "#df_b.to_csv(\"data/b.csv\")\n",
    "dtale.show(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the exact-ish match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe420bfaa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "exactish_result = link_exactish(df_a, df_b)\n",
    "dtale.show(exactish_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exactish_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://Davins-MBP:40000/dtale/iframe/8\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe43d666b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exactish_result.reset_index(level=0, inplace=True)\n",
    "\n",
    "exactish_joined = pd.merge(exactish_result, df_b, on=\"pk_b\")\n",
    "exactish_joined = pd.merge(exactish_joined, df_a, on=\"vendor_id\")\n",
    "\n",
    "#exactish_joined[\"vendor_id\"] = exactish_result.index[1]\n",
    "exactish_joined.to_csv(\"results/exactish_joined.csv\")\n",
    "exactish_joined_trimmed = exactish_joined[['NAME_A_NORM', 'NAME_B_NORM', 'STREET_A_NORM', 'STREET_B_NORM', 'CITY_A_NORM', 'CITY_B_NORM', \"POSTAL_A_NORM\", \"POSTAL_B_NORM\", 'PHONE_A_NORM', 'PHONE_B_NORM', \"WEB_A_NORM\", \"WEB_B_NORM\", \"vendor_id\", \"b_entity_id\", \"confidence_of_match\", \"name_norm_score\", \"street_norm_score\", \"city_norm_score\", \"phone_norm_score\", \"web_match_score\", \"postal_code_score\"]]\n",
    "exactish_joined_trimmed.to_csv(\"results/exactish_joined_trimmed.csv\")\n",
    "dtale.show(exactish_joined_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the fuzzy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "fuzzy_result = link_fuzzy(df_a, df_b)\n",
    "dtale.show(fuzzy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fuzzy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_result.reset_index(level=0, inplace=True)\n",
    "\n",
    "fuzzy_joined = pd.merge(fuzzy_result, df_b, on=\"pk_b\")\n",
    "fuzzy_joined = pd.merge(fuzzy_joined, df_a, on=\"vendor_id\")\n",
    "\n",
    "fuzzy_joined.to_csv(\"results/fuzzy_joined.csv\")\n",
    "fuzzy_joined_trimmed = fuzzy_joined[['NAME_A_NORM', 'NAME_B_NORM', 'STREET_A_NORM', 'STREET_B_NORM', 'CITY_A_NORM', 'CITY_B_NORM', \"POSTAL_A_NORM\", \"POSTAL_B_NORM\", 'PHONE_A_NORM', 'PHONE_B_NORM', \"WEB_A_NORM\", \"WEB_B_NORM\", \"vendor_id\", \"b_entity_id\", \"confidence_of_match\", \"name_norm_score\", \"street_norm_score\", \"city_norm_score\", \"phone_norm_score\", \"web_match_score\", \"postal_code_score\"]]\n",
    "fuzzy_joined_trimmed.to_csv(\"results/fuzzy_joined_trimmed.csv\")\n",
    "dtale.show(fuzzy_joined_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emit final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exactish_pairs = exactish_joined[[\"vendor_id\", \"b_entity_id\", \"confidence_of_match\"]]\n",
    "exactish_pairs.to_csv(\"results/exactish_pairs.csv\")\n",
    "fuzzy_pairs = fuzzy_joined[[\"vendor_id\", \"b_entity_id\", \"confidence_of_match\"]]\n",
    "fuzzy_pairs.to_csv(\"results/fuzzy_pairs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!\n",
    "Review the results in the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Munge",
   "language": "python",
   "name": "data_munge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
